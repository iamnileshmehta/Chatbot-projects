{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2sHq60Mx2dhFOq5RYQV74",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamnileshmehta/Chatbot-projects/blob/main/IGP_Building_a_Conversational_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cvlskrtzq9S",
        "outputId": "55f79a51-a487-4f99-acb0-ad70cef16950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-01 16:37:35--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2024-03-01 16:37:35--  https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  4.99MB/s    in 4m 44s  \n",
            "\n",
            "2024-03-01 16:42:19 (5.10 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install convokit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rmHHjVq8ilT",
        "outputId": "cf77e0ba-2fb3-4779-8d6e-8a7df2c74974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting convokit\n",
            "  Downloading convokit-3.0.0.tar.gz (183 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/183.2 kB\u001b[0m \u001b[31m986.2 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m143.4/183.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.3)\n",
            "Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
            "Collecting dill>=0.2.9 (from convokit)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n",
            "Collecting clean-text>=0.6.0 (from convokit)\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1 (from convokit)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.2)\n",
            "Collecting pymongo>=4.0 (from convokit)\n",
            "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n",
            "Collecting dnspython>=1.16.0 (from convokit)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.12.25)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.3.5->convokit) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.3.5->convokit) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.5)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=7168fafdbdf5a9feeb915515a0447db411dae7fdf0ef54055205e8bb825ed1b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=40ec29ee389d87365a0cb8aaecd2c3eade2bf285c463e713395edff6a35f3346\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n",
            "Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.8 dnspython-2.6.1 emoji-1.7.0 ftfy-6.1.3 msgpack-numpy-0.4.8 pymongo-4.6.2 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from convokit import Corpus, download\n",
        "corpus = Corpus(filename=download(\"movie-corpus\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_9_z05n8162",
        "outputId": "ef2f4306-229b-456b-e063-07d342afbd30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading movie-corpus to /root/.convokit/downloads/movie-corpus\n",
            "Downloading movie-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip (40.9MB)... Done\n",
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "default_backend: mem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/glove.twitter.27B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxcw_hI4_RHE",
        "outputId": "c3e270d6-18fc-439e-c265-3fa19cf1b6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6jY-lOkB_uu",
        "outputId": "75995daf-ef9a-45f8-8cfe-544726ffc669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-01 16:54:38--  http://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
            "Resolving zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)... 128.253.51.179\n",
            "Connecting to zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)|128.253.51.179|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip [following]\n",
            "--2024-03-01 16:54:38--  https://zissou.infosci.cornell.edu/convokit/datasets/movie-corpus/movie-corpus.zip\n",
            "Connecting to zissou.infosci.cornell.edu (zissou.infosci.cornell.edu)|128.253.51.179|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40854701 (39M) [application/zip]\n",
            "Saving to: ‘movie-corpus.zip’\n",
            "\n",
            "movie-corpus.zip    100%[===================>]  38.96M  50.1MB/s    in 0.8s    \n",
            "\n",
            "2024-03-01 16:54:39 (50.1 MB/s) - ‘movie-corpus.zip’ saved [40854701/40854701]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/movie-corpus.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFTA04kEDX6Y",
        "outputId": "17e0ca90-c100-4033-f932-995d78cfc3ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/movie-corpus.zip\n",
            "   creating: movie-corpus/\n",
            "  inflating: movie-corpus/utterances.jsonl  \n",
            "  inflating: movie-corpus/conversations.json  \n",
            "  inflating: movie-corpus/corpus.json  \n",
            "  inflating: movie-corpus/speakers.json  \n",
            "  inflating: movie-corpus/index.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "# Load and preprocess the Cornell Movie Dialogue Corpus from JSON files\n",
        "def load_dataset():\n",
        "    # Load conversations from JSON file\n",
        "    with open('/content/movie-corpus/conversations.json', 'r') as file:\n",
        "        conversations_data = json.load(file)\n",
        "\n",
        "    # Load lines from JSON file\n",
        "    with open('/content/movie-corpus/speakers.json', 'r') as file:\n",
        "        lines_data = json.load(file)\n",
        "\n",
        "    # Extract conversations and lines\n",
        "    conversations = conversations_data['conversations']\n",
        "    lines = {line['lineID']: line['text'] for line in lines_data['lines']}\n",
        "\n",
        "    # Create input-output pairs\n",
        "    inputs, outputs = [], []\n",
        "    for conv in conversations:\n",
        "        for i in range(len(conv) - 1):\n",
        "            inputs.append(lines[conv[i]])\n",
        "            outputs.append(lines[conv[i+1]])\n",
        "\n",
        "    return inputs, outputs\n",
        "\n",
        "inputs, outputs = load_dataset()\n",
        "\n",
        "# Tokenization and padding (similar to the previous code)\n",
        "# Define the model (similar to the previous code)\n",
        "# Compile and train the model (similar to the previous code)\n",
        "# Inference model (similar to the previous code)\n",
        "# Define inference function (similar to the previous code)\n",
        "# Test the chatbot (similar to the previous code)\n",
        "\n",
        "\n",
        "# Tokenization and padding\n",
        "tokenizer_inputs = Tokenizer()\n",
        "tokenizer_inputs.fit_on_texts(inputs)\n",
        "inputs_sequences = tokenizer_inputs.texts_to_sequences(inputs)\n",
        "max_len_input = max([len(seq) for seq in inputs_sequences])\n",
        "inputs_sequences = pad_sequences(inputs_sequences, maxlen=max_len_input, padding='post')\n",
        "\n",
        "tokenizer_outputs = Tokenizer()\n",
        "tokenizer_outputs.fit_on_texts(outputs)\n",
        "outputs_sequences = tokenizer_outputs.texts_to_sequences(outputs)\n",
        "max_len_output = max([len(seq) for seq in outputs_sequences])\n",
        "outputs_sequences = pad_sequences(outputs_sequences, maxlen=max_len_output, padding='post')\n",
        "\n",
        "# Define the model\n",
        "vocab_size_inputs = len(tokenizer_inputs.word_index) + 1\n",
        "vocab_size_outputs = len(tokenizer_outputs.word_index) + 1\n",
        "embedding_dim = 256\n",
        "latent_dim = 512\n",
        "\n",
        "encoder_inputs = tf.keras.Input(shape=(max_len_input,))\n",
        "encoder_embedding = Embedding(vocab_size_inputs, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = Bidirectional(LSTM(latent_dim, return_sequences=True, dropout=0.5))(encoder_embedding)\n",
        "encoder_lstm2 = LSTM(latent_dim, return_state=True, dropout=0.5)\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_lstm2)\n",
        "state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])\n",
        "state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.Input(shape=(None,))\n",
        "decoder_embedding = Embedding(vocab_size_outputs, embedding_dim, mask_zero=True)\n",
        "decoder_embedding = decoder_embedding(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, return_state=True, dropout=0.5)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_outputs, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit([inputs_sequences, outputs_sequences[:,:-1]], outputs_sequences[:,1:], batch_size=64, epochs=50, validation_split=0.2)\n",
        "\n",
        "# Inference model\n",
        "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = tf.keras.Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c = tf.keras.Input(shape=(latent_dim*2,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_inputs_single = tf.keras.Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = tf.keras.Model([decoder_inputs_single] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Token index to word dictionary\n",
        "reverse_word_index_inputs = {v: k for k, v in tokenizer_inputs.word_index.items()}\n",
        "reverse_word_index_outputs = {v: k for k, v in tokenizer_outputs.word_index.items()}\n",
        "\n",
        "# Define inference function\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer_outputs.word_index['<start>']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = reverse_word_index_outputs[sampled_token_index]\n",
        "        if sampled_word != '<end>':\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "        if (sampled_word == '<end>' or len(decoded_sentence.split()) > max_len_output):\n",
        "            stop_condition = True\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n",
        "# Test the chatbot\n",
        "def chat(input_text):\n",
        "    input_seq = tokenizer_inputs.texts_to_sequences([input_text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_len_input, padding='post')\n",
        "    response = decode_sequence(input_seq)\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "print(chat(\"Hello\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "33BZA-tG01Wx",
        "outputId": "47ad838e-1d4b-4209-bbdc-8dcafb3b9744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'conversations'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-80c7a3b69b16>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Tokenization and padding (similar to the previous code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-80c7a3b69b16>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Extract conversations and lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mconversations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversations_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conversations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lineID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'conversations'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "11Hcd5n801Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4yJ9tbl01QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EUxiKYsy01NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0qE9Kta01KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hjBAVjjP01Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_glove_embedding(file_path):\n",
        "    embedding_dict = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = [float(val) for val in values[1:]]\n",
        "            embedding_dict[word] = vector\n",
        "    return embedding_dict\n",
        "\n",
        "glove_file_path = '/content/glove.twitter.27B.200d.txt'  # Adjust the path accordingly\n",
        "glove_embedding = load_glove_embedding(glove_file_path)\n"
      ],
      "metadata": {
        "id": "rcwljoTV0fm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_and_convert_conversations(conversations, max_word_length):\n",
        "    filtered_conversations = []\n",
        "    for conversation in conversations:\n",
        "        filtered_dialogues = []\n",
        "        for dialogue in conversation:\n",
        "            if len(dialogue.split()) <= max_word_length:\n",
        "                filtered_dialogues.append(dialogue)\n",
        "        if len(filtered_dialogues) > 1:  # Ensure at least two dialogues for a conversation\n",
        "            filtered_conversations.append(filtered_dialogues)\n",
        "\n",
        "    input_texts = []\n",
        "    target_texts = []\n",
        "\n",
        "    for conversation in filtered_conversations:\n",
        "        for i in range(len(conversation) - 1):\n",
        "            input_text = '<start> ' + conversation[i]\n",
        "            target_text = conversation[i + 1] + ' <end>'\n",
        "            input_texts.append(input_text)\n",
        "            target_texts.append(target_text)\n",
        "\n",
        "    return input_texts, target_texts\n",
        "\n",
        "# Example usage:\n",
        "conversations = [\n",
        "    [\"Hello there!\", \"Hi, how are you?\", \"I'm good, thanks.\", \"That's great!\"],\n",
        "    [\"Hey!\", \"Hey, what's up?\", \"Not much, just chilling.\", \"Cool.\"],\n",
        "    [\"Good morning!\", \"Morning, how did you sleep?\", \"Not too bad, thanks for asking.\", \"No problem.\"]\n",
        "]\n",
        "\n",
        "max_word_length = 5\n",
        "\n",
        "input_texts, target_texts = filter_and_convert_conversations(conversations, max_word_length)\n",
        "\n",
        "print(\"Input Texts:\")\n",
        "print(input_texts)\n",
        "print(\"\\nTarget Texts:\")\n",
        "print(target_texts)\n"
      ],
      "metadata": {
        "id": "b5XDermoxA1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_target_dictionaries(target_texts):\n",
        "    target_word2id = {}\n",
        "    target_id2word = {}\n",
        "    for text in target_texts:\n",
        "        for word in text.split():\n",
        "            if word not in target_word2id:\n",
        "                id = len(target_word2id)\n",
        "                target_word2id[word] = id\n",
        "                target_id2word[id] = word\n",
        "    return target_word2id, target_id2word\n",
        "\n",
        "# Example usage:\n",
        "target_texts = [\n",
        "    \"how are you <end>\",\n",
        "    \"I am fine <end>\",\n",
        "    \"what about you <end>\"\n",
        "]\n",
        "\n",
        "target_word2id, target_id2word = create_target_dictionaries(target_texts)\n",
        "\n",
        "# Saving dictionaries as NumPy files\n",
        "np.save('target_word2id.npy', target_word2id)\n",
        "np.save('target_id2word.npy', target_id2word)\n"
      ],
      "metadata": {
        "id": "naqY5r3txakU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def prepare_input_data(sentences, word_embeddings, embedding_dim):\n",
        "    input_data = []\n",
        "    for sentence in sentences:\n",
        "        sentence_embedding = []\n",
        "        for word in sentence:\n",
        "            if word in word_embeddings:\n",
        "                word_embedding = word_embeddings[word]\n",
        "            else:\n",
        "                # If the word is not found in embeddings, use a zero vector\n",
        "                word_embedding = np.zeros(embedding_dim)\n",
        "            sentence_embedding.append(word_embedding)\n",
        "        input_data.append(sentence_embedding)\n",
        "    return input_data\n",
        "\n",
        "# Example usage:\n",
        "sentences = [\n",
        "    [\"This\", \"is\", \"a\", \"sample\", \"sentence\"],\n",
        "    [\"Another\", \"sentence\", \"for\", \"demonstration\"]\n",
        "]\n",
        "\n",
        "# Assuming 'glove_embedding' contains the GloVe embeddings loaded earlier\n",
        "embedding_dim = len(glove_embedding['word'])  # Assuming all words have the same embedding dimension\n",
        "\n",
        "input_data = prepare_input_data(sentences, glove_embedding, embedding_dim)\n"
      ],
      "metadata": {
        "id": "cvlYm5Fexagx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(input_data, batch_size):\n",
        "    num_batches = len(input_data) // batch_size\n",
        "    batches = []\n",
        "    for i in range(num_batches):\n",
        "        start_index = i * batch_size\n",
        "        end_index = start_index + batch_size\n",
        "        batch = input_data[start_index:end_index]\n",
        "        batches.append(batch)\n",
        "    # Handling the last batch which may have fewer elements\n",
        "    if len(input_data) % batch_size != 0:\n",
        "        last_batch = input_data[num_batches * batch_size:]\n",
        "        batches.append(last_batch)\n",
        "    return batches\n",
        "\n",
        "# Example usage:\n",
        "batch_size = 2\n",
        "\n",
        "batches = generate_batches(input_data, batch_size)\n"
      ],
      "metadata": {
        "id": "I5IupZxfxadw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "def build_model(input_shape, target_vocab_size, latent_dim):\n",
        "    # Define the encoder\n",
        "    encoder_inputs = Input(shape=input_shape)\n",
        "    encoder = LSTM(latent_dim, return_state=True)\n",
        "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Define the decoder\n",
        "    decoder_inputs = Input(shape=(None, target_vocab_size))\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "    # Dense layer to predict next token\n",
        "    decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage:\n",
        "input_shape = (None, embedding_dim)  # Shape of input data\n",
        "target_vocab_size = len(target_word2id)  # Size of the target vocabulary\n",
        "latent_dim = 256  # Dimensionality of the latent space\n",
        "\n",
        "model = build_model(input_shape, target_vocab_size, latent_dim)\n"
      ],
      "metadata": {
        "id": "IDynUFsvxaas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "7e8Lzm-TxaXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have input_data and target_data prepared\n",
        "# You need to reshape the target_data into one-hot encoding format\n",
        "\n",
        "# Assuming you have the model trained and ready\n",
        "\n",
        "# Generate predictions\n",
        "predictions = model.predict([input_data, target_data])\n",
        "\n",
        "# Now you have predictions for each input-output pair\n"
      ],
      "metadata": {
        "id": "zEExFtLTxaUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E61gCd3qxaRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}